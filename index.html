
<html>
<head>
<title>Jeremy Cohen</title>
</head>
<body style="padding: 10px;">
<h2>Jeremy Cohen</h2>
<div style="width:1000px">
<div>
<img src="http://zicokolter.com/img/jeremycohen.jpg" style="width: 200px; float: left; padding-right: 15px">
<p>I'm a PhD student in the machine learning department at CMU, co-advised by <a href="http://zicokolter.com">Zico Kolter</a> and <a href="https://www.cs.cmu.edu/~atalwalk">Ameet Talwalkar</a>.
  My main research interest is turning the practice of deep leaning into a mature engineering discipline. </p>
<a href="https://scholar.google.com/citations?user=r493814AAAAJ">Google Scholar</a>
<p>Email spam is known to be a big problem these days, so like other academics I obfuscate <a href="email.jpg">my email address</a>.
<p style="color: white">My email address is jeremycohen@cmu.edu</p>
</div>
<div style="clear: left; padding-top: 10px;">
<h3>Conference publications:</h3>

<ol>
  <li>
    <a href="https://openreview.net/forum?id=jh-rTtvkGeM"><b>Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability.</b></a>
    <br />Jeremy Cohen, Simran Kaur, Yuanzhi Li, Zico Kolter, and Ameet Talwalker.
    In ICLR 2021.
    <br /><br />
    We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the leading eigenvalue of the training loss Hessian hovers just above the value 2 / (step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability.
    <p />
    <img src="gd.gif" width="800px"/>
  </li>
<br />

<li>
  <a href="https://arxiv.org/abs/1902.02918"><b>Certified Adversarial Robustness via Randomized Smoothing.</b></a>
  <br />
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.
In ICML 2019.
<br>
<a href="https://arxiv.org/abs/1902.02918">arXiv</a>, <a href="https://github.com/locuslab/smoothing">Code</a>, <a href="https://www.facebook.com/icml.imls/videos/607431793098200?t=2270">short ICML talk</a>, <a href="https://www.youtube.com/watch?v=UHs2mGBH0Fg?t=1380">Zico's Simons Talk</a>

<p>Extending <a href="https://arxiv.org/abs/1802.03471">recent</a> <a href="https://arxiv.org/abs/1809.03113">work</a>, we show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is provably robust to perturbations in L2 norm.  This method is the only provable adversarial defense that scales to ImageNet.  It also outperforms all other provable L2 adversarial defenses on CIFAR-10 by a wide margin.  Best of all, the method is extremely simple to implement and to understand.</p>
<img src="https://raw.githubusercontent.com/locuslab/smoothing/master/figures/panda_0.25.gif"/>
<img src="https://raw.githubusercontent.com/locuslab/smoothing/master/figures/panda_0.50.gif"/>
<img src="https://raw.githubusercontent.com/locuslab/smoothing/master/figures/panda_1.00.gif"/>
</li>
</ol>
</div>
</div>
</body>
</html>
