
<html>
<head>
<title>Jeremy Cohen</title>
</head>
<body style="padding: 10px;">
<h2>Jeremy Cohen</h2>
<div style="width:1000px">
<div>
<img src="http://zicokolter.com/img/jeremycohen.jpg" style="width: 200px; float: left; padding-right: 15px">
<p>I'm currently a Research Fellow at the <a href="https://www.simonsfoundation.org/flatiron/">Flatiron Institute</a>, in the Center for Computational Mathematics.  Previously, I did a PhD in the machine learning department at Carnegie Mellon, co-advised by <a href="http://zicokolter.com">Zico Kolter</a> and <a href="https://www.cs.cmu.edu/~atalwalk">Ameet Talwalkar</a>.</p>
<p>My research aims to <b>connect theory and practice in deep learning</b>. The ultimate goal is to turn deep learning from a kind of alchemy into a mature engineering discipline.</p>
<p>I am currently studying <b>the dynamics of optimization in deep learning</b>.  Neural networks are trained using optimization algorithms, and understanding the behavior of these algorithms will help us understand and improve the training process.</p>
<p>My current email is <span style="font-family: monospace;">jcohen@flatironinstitute.org</span>. See also <a href="https://scholar.google.com/citations?user=r493814AAAAJ">Google Scholar</a> and <a href="https://x.com/deepcohen">Twitter</a>.
</div>
<div style="clear: left; padding-top: 10px;">
<h3>Publications:</h3>

<ol>
    <li>
        <a href="https://openreview.net/forum?id=sIE2rI3ZPs"><b>Understanding Optimization in Deep Learning with Central Flows.</b></a>
        <br />Jeremy Cohen*, Alex Damian*, Ameet Talwalkar, Zico Kolter, and Jason D. Lee.
        <br/>ICLR 2025.
        <br /><br />
        This paper develops a methodology for analyzing the oscillatory dynamics of optimization algorithms in deep learning.  The basic idea is to analyze an optimization algorithm by deriving a differential equation called a "central flow" that directly models the time-averaged (i.e. smoothed) trajectory of the optimizer.  This differential equation renders <i>explicit</i> what was previously <i>implicit</i> in the optimizer's oscillatory dynamics.
        <p />
        <img src="images/overview.gif" width="800px"/>
      </li>
    <li>
        <a href="https://arxiv.org/abs/2207.14484"><b>Adaptive Gradient Methods at the Edge of Stability.</b></a>
        <br />Jeremy Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E. Dahl, and Justin Gilmer.
        <br/>ArXiv 2023.
        <br /><br />
        This paper shows that the edge of stability phenomenon extends to adaptive gradient methods such as Adam.
        <p />
      </li>

  <li>
    <a href="https://arxiv.org/abs/2103.00065"><b>Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability.</b></a>
    <br />Jeremy Cohen, Simran Kaur, Yuanzhi Li, Zico Kolter, and Ameet Talwalker.
    <br/>ICLR 2021.
    <br /><br/>
    This paper demonstrates that full-batch gradient descent on neural network training objectives typically operates in a regime called the Edge of Stability. In this regime, the leading eigenvalue of the training loss Hessian hovers just above the value 2 / (step size), and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training.
    <p />
    <img src="images/gd.gif" width="800px"/>
  </li>
<br />

<li>
  <a href="https://arxiv.org/abs/1902.02918"><b>Certified Adversarial Robustness via Randomized Smoothing.</b></a>
  <br />
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.
<br />
ICML 2019
<br>
<a href="https://arxiv.org/abs/1902.02918">arXiv</a>, <a href="https://github.com/locuslab/smoothing">Code</a>, <a href="https://www.facebook.com/icml.imls/videos/607431793098200?t=2270">short ICML talk</a>, <a href="https://www.youtube.com/watch?v=UHs2mGBH0Fg?t=1380">Zico's Simons Talk</a>

<p>Extending <a href="https://arxiv.org/abs/1802.03471">recent</a> <a href="https://arxiv.org/abs/1809.03113">work</a>, we show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is provably robust to perturbations in L2 norm.  This method is the only provable adversarial defense that scales to ImageNet.  It also outperforms all other provable L2 adversarial defenses on CIFAR-10 by a wide margin.  Best of all, the method is extremely simple to implement and to understand.</p>
<img src="https://raw.githubusercontent.com/locuslab/smoothing/master/figures/panda_0.25.gif"/>
<img src="https://raw.githubusercontent.com/locuslab/smoothing/master/figures/panda_0.50.gif"/>
<img src="https://raw.githubusercontent.com/locuslab/smoothing/master/figures/panda_1.00.gif"/>
</li>
</ol>
</div>
</div>
</body>
</html>
